{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Import Library\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tweepy as tweepy\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime, date, timedelta\n",
    "#Count for common words\n",
    "from collections import Counter\n",
    "#Tweet pre-processor\n",
    "import preprocessor as p\n",
    "#NLTK\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "#Loading NLTK\n",
    "import nltk\n",
    "from nltk import word_tokenize, pos_tag, pos_tag_sents\n",
    "from nltk.corpus import stopwords\n",
    "#TextBlob\n",
    "from textblob import TextBlob\n",
    "#Stanza\n",
    "import stanza\n",
    "#stanza.download('en')\n",
    "#Stanford CoreNLP\n",
    "from pycorenlp import StanfordCoreNLP\n",
    "#Spacy \n",
    "import spacy\n",
    "#Word cloud\n",
    "#pip install wordcloud\n",
    "from wordcloud import WordCloud\n",
    "#Creating nlp object\n",
    "#Dowload package first from terminal of environment acaconda: python -m spacy download en_core_web_sm\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "#ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "#VADER\n",
    "#pip install vaderSentiment\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "#Sentiwordnet\n",
    "#nltk.download('sentiwordnet')\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "#wordnet\n",
    "nltk.download('wordnet')\n",
    "from nltk.corpus import wordnet\n",
    "#pip install nest_asyncio\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import twint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Crawl data from twitter with twin  Python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crawl > 10.000 data from twitter using twin with keyword =\"covid19 covid vaccine\"\n",
    "config = twint.Config()\n",
    "config.Search = \"covid19 covid vaccine\"\n",
    "config.Limit = 10000\n",
    "config.Lang = \"en\"\n",
    "config.Store_csv = True\n",
    "config.Output = \"./datavaccine-twitter-new.csv\"\n",
    "config.Pandas = True\n",
    "# Run\n",
    "twint.run.Search(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Read csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Import Data\n"
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, date, timedelta\n",
    "df = pd.read_csv(\"./data/vaccine-twitter.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Lọc data\n"
    }
   },
   "outputs": [],
   "source": [
    "df = df[(df['tweet'].str.contains(\"vacc\"))\n",
    "                            | (df['tweet'].str.contains(\"Vacc\"))\n",
    "                            | (df['hashtags'].str.contains(\"vacc\"))\n",
    "                            | (df['hashtags'].str.contains(\"Vacc\"))]\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Pre-processing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# # 2.1 Remove punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%% Lọc hastag\n"
    }
   },
   "outputs": [],
   "source": [
    "import string\n",
    "string.punctuation\n",
    "def processTweet(tweet):\n",
    "    # Remove HTML special entities (e.g. &amp;)\n",
    "    tweet = re.sub(r'\\&\\w*;', '', tweet)\n",
    "    #Convert @username to AT_USER\n",
    "    tweet = re.sub('@[^\\s]+','',tweet)\n",
    "    # Remove tickers\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    "    # To lowercase\n",
    "    tweet = tweet.lower()\n",
    "    # Remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*\\/\\w*', '', tweet)\n",
    "    # Remove hashtags\n",
    "    tweet = re.sub(r'#\\w*', '', tweet)\n",
    "    # Remove Punctuation and split 's, 't, 've with a space for filter\n",
    "    #tweet = re.sub(r'[' + punctuation.replace('@', '') + ']+', ' ', tweet)\n",
    "    # Remove words with 2 or fewer letters\n",
    "    tweet = re.sub(r'\\b\\w{1,2}\\b', '', tweet)\n",
    "    # Remove whitespace (including new line characters)\n",
    "    tweet = re.sub(r'\\s\\s+', ' ', tweet)\n",
    "    # Remove single space remaining at the front of the tweet.\n",
    "    tweet = tweet.lstrip(' ') \n",
    "    # Remove characters beyond Basic Multilingual Plane (BMP) of Unicode:\n",
    "    tweet = ''.join(c for c in tweet if c <= '\\uFFFF') \n",
    "    return tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_puntuation_tweet'] = df['tweet'].apply(processTweet)\n",
    "# preview some cleaned tweets\n",
    "df['cleaned_puntuation_tweet'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save as CSV file\n",
    "df.to_csv('./data/covid_vaccine_tweets_extracted_20211106_204713.csv')\n",
    "#jupyter notebook --NotebookApp.iopub_data_rate_limit=10000000000 from terminal if it has error: data exceeded\n",
    "df = pd.read_csv('./data/covid_vaccine_tweets_extracted_20211106_204713.csv', )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Remove stop words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "stop_words.update([\"new\", \"total\", \"due\", \"first\",\"amp\", \"vaccines\", \"covid\", \"get\", \"today\", \"nhs\", \"india\",\"nwo\", \"nhs\", \"additional\",\n",
    "                   \"biden\",\"national\", \"usfda\", \"adverse\", \"clinical\", \"deltavariant\", \"read\", \"dose\", \"fda\", \"cdc\", \"dont\", \"dose\",\n",
    "                  \"least\", \"daily\", \"many\", \"current\", \"second\", \"last\", \"long\", \"next\", \"severe\", \"third\",\"read\", \"receive\",\n",
    "                 \"next\", \"full\", \"covid19vaccine\", \"natural\", \"media\", \"old\", \"young\", \"public\", \"global\", \"covid19vic\"\n",
    "                  , \"early\", \"different\", \"local\", \"social\", \"much\", \"true\", \"ready\", \"federal\", \"medical\"])\n",
    "df['removed_stopwords_tweet'] = df['cleaned_puntuation_tweet'].apply(lambda x: ' '.join([word for word in x.split()\n",
    "                                                                        if word not in stop_words]))\n",
    "print(df['removed_stopwords_tweet'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.3 Steam word - Lemmatazation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['lemma_tweet'] = df['removed_stopwords_tweet'].apply(lambda row: \" \".join([w.lemma_ for w in nlp(row)]))\n",
    "print(df['lemma_tweet'].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# #  2.4 Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Break text paragraph into words\n",
    "tweet_df = df['removed_stopwords_tweet']\n",
    "tokenized_df = []\n",
    "for row in tweet_df:\n",
    "    word_tokenized_in_line = word_tokenize(row)\n",
    "    tokenized_df.append(word_tokenized_in_line)\n",
    "df['tokenized_tweet'] = tokenized_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 POS(part of speech)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# POS tagger dictionary\n",
    "pos_dict = {'J':wordnet.ADJ, 'V':wordnet.VERB, 'N':wordnet.NOUN, 'R':wordnet.ADV}\n",
    "\n",
    "def token_stop_pos(text):\n",
    "    tags = pos_tag(word_tokenize(text))\n",
    "    newlist = []\n",
    "    for word, tag in tags:\n",
    "        if word.lower() not in set(stopwords.words('english')):\n",
    "            newlist.append(tuple([word, pos_dict.get(tag[0])]))\n",
    "    return newlist\n",
    "\n",
    "df['tagged_POS_tweet'] = df['removed_stopwords_tweet'].apply(token_stop_pos)\n",
    "print(df.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count for common adj\n",
    "word_counts = Counter(line for line in df['tagged_POS_tweet'] for line in set(line) if line[1] == 'a')\n",
    "common_words= word_counts.most_common(58)\n",
    "# print(word_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. BOW and IF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#BoW\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "corpus = ['Covid vaccine is good.', 'Covid vaccine is good and important.', 'Covid vaccine is safe.']\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(vectorizer.get_feature_names())\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TF-IDF VECTORIZATION\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(corpus)\n",
    "print(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#MOST COMMON WORDS IN TWITTER DATASET\n",
    "print(\"MOST COMMON WORDS IN TWITTER DATASET:\")\n",
    "all_words = []\n",
    "for line in list(df['removed_stopwords_tweet']):\n",
    "    words = line.split()\n",
    "    for word in words:\n",
    "        all_words.append(word.lower())\n",
    "   \n",
    "Counter(all_words).most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT word frequency distribution of first few words\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.xticks(fontsize=13, rotation=90)\n",
    "fd = nltk.FreqDist(all_words)\n",
    "fd.plot(20,cumulative=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Sentiment Analysis with 3 models: Textblob, Vader and Stanza\n",
    "#### SA Using Textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_compare = 0.05\n",
    "# function to calculate subjectivity\n",
    "def getSubjectivity(words):\n",
    "    return TextBlob(words).sentiment.subjectivity\n",
    "# function to calculate polarity\n",
    "def getPolarity(words):\n",
    "        return TextBlob(words).sentiment.polarity\n",
    "\n",
    "# function to analyze the reviews\n",
    "def analysis(score):\n",
    "    if score <= -(v_compare):\n",
    "        return 'Negative'\n",
    "    elif score >= v_compare:\n",
    "        return 'Positive'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "df = df[['removed_stopwords_tweet', 'lemma_tweet']]\n",
    "df['polarity'] = df['lemma_tweet'].apply(getPolarity) \n",
    "df['analysis'] = df['polarity'].apply(analysis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of positive, negative, neutral tweets\n",
    "tb_counts = df.analysis.value_counts()\n",
    "print(tb_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Visualize\n",
    "tb_count= df.analysis.value_counts()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.pie(tb_counts.values, labels = tb_count.index, explode = (0, 0, 0.25), autopct='%1.1f%%', shadow=False)\n",
    "plt.legend()\n",
    "plt.title(\"Textblob Sentiment Result\")\n",
    "plt.savefig('./img/SA_Textblob')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SA Using VADER(Valence Aware Dictionary and Sentiment Reasoner)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer = SentimentIntensityAnalyzer()\n",
    "# function to calculate vader sentiment\n",
    "def vadersentimentanalysis(tweet):\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    return vs['compound']\n",
    "df['vader_sentiment'] = df['lemma_tweet'].apply(vadersentimentanalysis)\n",
    "# function to analyse\n",
    "def vader_analysis(compound):\n",
    "    if compound >= v_compare:\n",
    "        return 'Positive'\n",
    "    elif compound <= -(v_compare) :\n",
    "        return 'Negative'\n",
    "    else:\n",
    "        return 'Neutral'\n",
    "df['vader_analysis'] = df['vader_sentiment'].apply(vader_analysis)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of positive, negative, neutral tweets\n",
    "vader_counts = df['vader_analysis'].value_counts()\n",
    "vader_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vader_counts= df['vader_analysis'].value_counts()\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.pie(vader_counts.values, labels = vader_counts.index, explode = (0.1, 0, 0), autopct='%1.1f%%', shadow=False)\n",
    "plt.legend()\n",
    "plt.title(\"Vader Sentiment Result\")\n",
    "plt.savefig('./img/SA_Vader')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sentiment Analysis with Stanza"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Minus 1 so as to bring score range of [0,2] to [-1,1]\n",
    "def stanza_analyze(Text):\n",
    "    document = nlp(Text)\n",
    "    print('Processing')\n",
    "    return np.mean([(i.sentiment - 1) for i in document.sentences]) \n",
    "# Obtain sentiment categorical score generated by Stanza\n",
    "df['stanza_score'] = df['lemma_tweet'].apply(lambda x: stanza_analyze(x))\n",
    "# Convert average Stanza sentiment score into sentiment categories\n",
    "df['stanza_sentiment'] = df['stanza_score'].apply(lambda c: 'Positive' if c >= v_compare else\n",
    "                                                  ('Negative' if c <= -(v_compare) else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neutral_thresh = 0.05\n",
    "# Convert average Stanza sentiment score into sentiment categories\n",
    "df['stanza_sentiment'] = df['stanza_score'].apply(lambda c: 'Positive' if c >= v_compare else ('Negative' if c <= -(v_compare) else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stanza_counts= df['stanza_sentiment'].value_counts()\n",
    "stanza_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.pie(stanza_counts.values, labels = stanza_counts.index, explode = (0.1, 0, 0), autopct='%1.1f%%')\n",
    "plt.legend()\n",
    "plt.title(\"Stanza Sentiment Result\")\n",
    "plt.savefig('./img/SA_stanza')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visual representation of TextBlob, VADER, Stanza results by Bar chart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiments = pd.concat([tb_counts, \n",
    "                           vader_counts, \n",
    "                           stanza_counts\n",
    "                          ]).reset_index(drop=True)\n",
    "df_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to get value counts\n",
    "def get_value_counts(col_name, analyzer_name):\n",
    "    count = pd.DataFrame(df[col_name].value_counts())\n",
    "    percentage = pd.DataFrame(df[col_name].value_counts(normalize=True).mul(100))\n",
    "    value_counts_df = pd.concat([count, percentage], axis = 1)\n",
    "    value_counts_df = value_counts_df.reset_index()\n",
    "    value_counts_df.columns = ['sentiment', 'counts', 'percentage']\n",
    "    value_counts_df.sort_values('sentiment', inplace = True)\n",
    "    value_counts_df['percentage'] = value_counts_df['percentage'].apply(lambda x: round(x,2))\n",
    "    value_counts_df = value_counts_df.reset_index(drop = True)\n",
    "    value_counts_df['analyzer'] = analyzer_name\n",
    "    return value_counts_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SA by VADER\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# Obtaining NLTK scores\n",
    "df['nltk_scores'] = df['lemma_tweet'].apply(lambda x: sia.polarity_scores(x))\n",
    "\n",
    "# Obtaining NLTK compound score\n",
    "df['nltk_cmp_score'] = df['nltk_scores'].apply(lambda score_dict: score_dict['compound'])\n",
    "neutral_thresh = 0.05\n",
    "# Categorize scores into the sentiments of positive, neutral or negative\n",
    "df['nltk_sentiment'] = df['nltk_cmp_score'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))\n",
    "nltk_sentiment_df = get_value_counts('nltk_sentiment','NLTK Vader')\n",
    "nltk_sentiment_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SA by TextBlob\n",
    "# Obtain polarity scores generated by TextBlob\n",
    "df['textblob_score'] = df['lemma_tweet'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "neutral_thresh = 0.05\n",
    "# Convert polarity score into sentiment categories\n",
    "df['textblob_sentiment'] = df['textblob_score'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))\n",
    "textblob_sentiment_df = get_value_counts('textblob_sentiment','TextBlob')\n",
    "textblob_sentiment_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SA with Stanza\n",
    "nlp = stanza.Pipeline(lang='en', processors='tokenize,sentiment')\n",
    "def stanza_analyze(Text):\n",
    "    document = nlp(Text)\n",
    "    print('Processing')\n",
    "    return np.mean([(i.sentiment - 1) for i in document.sentences]) # Minus 1 so as to bring score range of [0,2] to [-1,1]\n",
    "# Obtain sentiment categorical score generated by Stanza\n",
    "df['stanza_score'] = df['lemma_tweet'].apply(lambda x: stanza_analyze(x))\n",
    "neutral_thresh = 0.05\n",
    "# Convert average Stanza sentiment score into sentiment categories\n",
    "df['stanza_sentiment'] = df['stanza_score'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))\n",
    "stanza_sentiment_df = get_value_counts('stanza_sentiment','Stanza')\n",
    "stanza_sentiment_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Insights from SA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiments = pd.concat([nltk_sentiment_df, \n",
    "                           textblob_sentiment_df, \n",
    "                           stanza_sentiment_df\n",
    "                          ]).reset_index(drop=True)\n",
    "df_sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_sentiments_pivot = df_sentiments.pivot(index='sentiment', columns='analyzer', values='percentage')\n",
    "df_sentiments_pivot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,6))\n",
    "ax = sns.barplot(x=\"analyzer\", y=\"percentage\",\n",
    "                 hue=\"sentiment\", data=df_sentiments)\n",
    "\n",
    "# Display annotations\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{round(p.get_height(),1)}%\", \n",
    "                   (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "                   ha = 'center', va = 'center', \n",
    "                   size=12,\n",
    "                   xytext = (0, -12), \n",
    "                   textcoords = 'offset points')\n",
    "plt.savefig(\"./img/SA_all_BarChart\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['composite_scores'] = (df['polarity']+df['vader_sentiment']+df['stanza_score'])/3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['composite_scores'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_counts= df['composite_vote'].value_counts()\n",
    "composite_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Threshold for neutral sentiment\n",
    "neutral_thresh = 0.05\n",
    "# Convert average sentiment score (from all 3 analyzers) into sentiment categories\n",
    "df['composite_vote'] = df['composite_scores'].apply(lambda c: 'Positive' if c >= neutral_thresh else ('Negative' if c <= -(neutral_thresh) else 'Neutral'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.pie(stanza_counts.values, labels = composite_counts.index, explode = (0.1, 0, 0), autopct='%1.1f%%')\n",
    "plt.legend()\n",
    "plt.title(\"Composite Result\")\n",
    "plt.savefig('./img/composite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make use of sentiments from NLTK Vader, TextBlob and Stanza\n",
    "df['sentiment_votes'] =  df.apply(lambda x: list([x['analysis'], \n",
    "                                                                x['vader_analysis'], \n",
    "                                                                x['stanza_sentiment']]),axis=1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function to get sentiment that appears most often amongst the 3 votes\n",
    "def get_most_voted_senti(List):\n",
    "    if len(List) == len(set(List)): # If all elements are different\n",
    "        return 'Neutral'\n",
    "    else:\n",
    "        return max(set(List), key = List.count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get composite sentiment vote\n",
    "df['composite_vote'] = df['sentiment_votes'].apply(lambda x: get_most_voted_senti(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "composite_sentiment_count = df['composite_vote'].value_counts()\n",
    "composite_sentiment_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.pie(composite_sentiment_count.values, labels = composite_counts.index, explode = (0.1, 0, 0), autopct='%1.1f%%')\n",
    "plt.legend()\n",
    "plt.title(\"Composite Sentiment Result\")\n",
    "plt.savefig('./img/SA_Composite')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
